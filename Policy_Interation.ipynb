{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Value_Iteration_Gym.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "5a8936c6-b42d-4ff4-8bca-f01e68eba367"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "5f442db3-8030-4d20-e7dd-41805b2abbd5"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "033d9179-5ab3-4435-f790-2d41a059d245"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "b38f4506-7e30-4769-8464-f66a964f88b3"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "565f1d47-36f5-4d6a-8621-cd88a75602f0"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play(env, policy_1, True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "20783c10-880a-44a0-9cb9-a02e4e4fad3f"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "57509226-867f-4851-cc59-565977a54a1b"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "70578b92-c067-47df-8a47-9542607b8822"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "70e54b7d-0d98-4efb-8f93-2e5268362973"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 55/1000\n",
            "Average number of steps: 11.272727272727273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "31a21107-1060-4984-9913-11fa6c6f475d"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 103/1000\n",
            "Average number of steps: 14.320388349514563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "310efab1-e643-4f25-cda1-e652ec5ff51c"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 727/1000\n",
            "Average number of steps: 37.95185694635488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kskvThA0UYQa"
      },
      "source": [
        "# Policy - Interation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    return v_values\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "42a7e11b-6d9f-41f4-ba29-a5a818360370"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "44237398-1dc1-42fe-9ae3-a7226d1d005b"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "b3189035-91af-4378-a243-5b73750a2243"
      },
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "c700ab38-424b-40ab-fa9e-3e1b34f310db"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "b4d96deb-151f-49f3-d6d5-ec8b50a61f83"
      },
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M89k-VZwcWFS"
      },
      "source": [
        "def policy_iteration(env,max_iters=500, gamma=0.9):\n",
        "    # Initiallize \n",
        "      policy = np.ones(env.observation_space.n, dtype=np.int)\n",
        "      for i in range(max_iters):\n",
        "        # Compute the Value Function for the current policy\n",
        "        v_values = policy_evaluation(env,policy,max_iters=500,gamma=0.9)\n",
        "        policy_stable = True\n",
        "        #loop each state\n",
        "        for state in range(env.observation_space.n):\n",
        "          old_action = policy[state]\n",
        "          q_values = []\n",
        "          # loop through each action\n",
        "          for action in range(env.action_space.n):\n",
        "              # loop each possible outcome\n",
        "              q_value = 0\n",
        "              for prob, next_state, reward, done in env.P[state][action]:\n",
        "                   q_value += prob * (reward + gamma * v_values[next_state])\n",
        "              q_values.append(q_value)\n",
        "          # select the best action\n",
        "          best_action = np.argmax(q_values)\n",
        "          policy[state] = best_action\n",
        "\n",
        "          if old_action!=best_action:\n",
        "            policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "          return policy,v_values"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65qPuZD8UdTP"
      },
      "source": [
        "# Values - Interation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "        \n",
        "    return policy"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            \n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    policy = policy_extraction(env,v_values,gamma)\n",
        "    return policy,v_values"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "6b29ddef-4f36-4609-b576-400fe058cd84"
      },
      "source": [
        "optimal_policy,optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "efabc15a-c0b8-44d9-dbd5-e5909d8fe8c2"
      },
      "source": [
        "optimal_v_values"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGCF4G7XErH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139aa3db-daa0-4ec4-d4bb-9d30c31e8a64"
      },
      "source": [
        "optimal_policy"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "c32c1e46-b997-4a1e-df62-c2ca2204fd7e"
      },
      "source": [
        "optimal_policy\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 750/1000\n",
            "Average number of steps: 36.010666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbxiTfkpTjrH"
      },
      "source": [
        "# Compare policy- iteration and values-iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcYV5xbSZAHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83335b99-5586-4dd4-a47d-240938e05d58"
      },
      "source": [
        "# Enviroment 'FrozenLake-v0'\n",
        "environment = gym.make('FrozenLake-v0')\n",
        "n_esp = 1000\n",
        "# Policy Interation\n",
        "print('Policy Interation')\n",
        "start_time = time.time()\n",
        "optimal_policy, optimal_v_values = policy_iteration(environment,max_iters=500, gamma=0.9)\n",
        "end_time = time.time()\n",
        "print('Time for run policy-iteration', end_time-start_time)\n",
        "play_multiple_times(environment,optimal_policy,n_esp)\n",
        "\n",
        "# Values Interation\n",
        "print('Values Interation')\n",
        "start_time = time.time()\n",
        "optimal_policy, optimal_v_values = value_iteration(environment,max_iters=500, gamma=0.9)\n",
        "end_time = time.time()\n",
        "print('Time for run values-iteration', end_time-start_time)\n",
        "play_multiple_times(environment,optimal_policy,n_esp)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Interation\n",
            "Converged at 32-th iteration.\n",
            "Converged at 80-th iteration.\n",
            "Time for run policy-iteration 0.019724369049072266\n",
            "Number of successes: 705/1000\n",
            "Average number of steps: 38.299290780141845\n",
            "Values Interation\n",
            "Converged at 79-th iteration.\n",
            "Time for run values-iteration 0.05530142784118652\n",
            "Number of successes: 731/1000\n",
            "Average number of steps: 37.40766073871409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBy_06ih6mrt",
        "outputId": "e325f407-8297-44ff-beae-e55dccbd818f"
      },
      "source": [
        "# Enviroment 'FrozenLake8x8-v0'\n",
        "environment = gym.make('FrozenLake8x8-v0')\n",
        "n_esp = 1000\n",
        "# Policy Interation\n",
        "print('Policy Interation')\n",
        "start_time = time.time()\n",
        "optimal_policy, optimal_v_values = policy_iteration(environment,max_iters=500, gamma=0.9)\n",
        "end_time = time.time()\n",
        "print('Time for run policy-iteration', end_time-start_time)\n",
        "play_multiple_times(environment,optimal_policy,n_esp)\n",
        "\n",
        "# Values Interation\n",
        "print('Values Interation')\n",
        "start_time = time.time()\n",
        "optimal_policy, optimal_v_values = value_iteration(environment,max_iters=500, gamma=0.9)\n",
        "end_time = time.time()\n",
        "print('Time for run values-iteration', end_time-start_time)\n",
        "play_multiple_times(environment,optimal_policy,n_esp)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Interation\n",
            "Converged at 46-th iteration.\n",
            "Converged at 121-th iteration.\n",
            "Converged at 117-th iteration.\n",
            "Time for run policy-iteration 0.09007978439331055\n",
            "Number of successes: 731/1000\n",
            "Average number of steps: 71.71135430916553\n",
            "Values Interation\n",
            "Converged at 117-th iteration.\n",
            "Time for run values-iteration 0.20308589935302734\n",
            "Number of successes: 735/1000\n",
            "Average number of steps: 71.80272108843538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfW_f3Zh7Hvq",
        "outputId": "970e05ee-8804-4284-db34-1efd6b31db2f"
      },
      "source": [
        "# Enviroment 'Taxi-v3'\n",
        "environment = gym.make('Taxi-v3')\n",
        "n_esp = 1000\n",
        "# Policy Interation\n",
        "print('Policy Interation')\n",
        "start_time = time.time()\n",
        "optimal_policy, optimal_v_values = policy_iteration(environment,max_iters=500, gamma=0.9)\n",
        "end_time = time.time()\n",
        "print('Time for run policy-iteration', end_time-start_time)\n",
        "play_multiple_times(environment,optimal_policy,n_esp)\n",
        "\n",
        "# Values Interation\n",
        "print('Values Interation')\n",
        "start_time = time.time()\n",
        "optimal_policy, optimal_v_values = value_iteration(environment,max_iters=500, gamma=0.9)\n",
        "end_time = time.time()\n",
        "print('Time for run values-iteration', end_time-start_time)\n",
        "play_multiple_times(environment,optimal_policy,n_esp)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Interation\n",
            "Converged at 88-th iteration.\n",
            "Converged at 97-th iteration.\n",
            "Converged at 100-th iteration.\n",
            "Converged at 101-th iteration.\n",
            "Converged at 102-th iteration.\n",
            "Converged at 103-th iteration.\n",
            "Converged at 106-th iteration.\n",
            "Converged at 109-th iteration.\n",
            "Converged at 110-th iteration.\n",
            "Converged at 111-th iteration.\n",
            "Converged at 112-th iteration.\n",
            "Converged at 115-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Time for run policy-iteration 2.8494961261749268\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.157\n",
            "Values Interation\n",
            "Converged at 116-th iteration.\n",
            "Time for run values-iteration 1.4531328678131104\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iNC3wOiRQ8i"
      },
      "source": [
        "# Nhận xét"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBqVRmInRZZ6"
      },
      "source": [
        "Thông qua kết quả trả về có thể thấy value-iteration trả về kết quả tốt hơn policy-iteration với số trường hợp thành công lớn hơn và số bước đi ít hơn policy - iteration. Ngược lại policy-iteration thường có thời gian đưa ra kết quả nhanh hơn value-iteration vì policy-iteration hội tụ trong ít lần lặp hơn value-iteration(value-iteration nặng hơn về mặt tính toán). Nhưng nhìn chung việc policy-iteration và value-iteration cái nào nhanh hơn còn phụ thuộc vào môi trường, với trường hợp env 'FrozenLake-v0' và 'FrozenLake8x8-v0' policy-iteration nhanh hơn values - iteration nhưng với env Taxi v3 policy-iteration cho ra kết quả chậm hơn và cả 2 đều đưa ra kết quả tốt"
      ]
    }
  ]
}